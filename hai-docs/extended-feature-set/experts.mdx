---
title: "Experts"
description: "Learn how Experts help tailor the HAI Code Generator with domain-specific context for smarter, more accurate code generation."
---

### ğŸ¤” 1. What are Experts?

Experts are like smart guides for the code generatorâ€”they help it focus on specific tech stacks or domain-specific knowledge the LLM may not know on its own.

### ğŸ’¡ 2. Why do you need an Expert?

Imagine this: youâ€™ve got custom UI components or internal libraries with secret documentation.

The LLM? Itâ€™s smart, but it doesnâ€™t know your internal playbook.

Experts bring that knowledge into the loop, so your code gen becomes context-aware and more accurate.

### ğŸ› ï¸ 3. Built-in Experts

Weâ€™ve bundled in 4 ready-to-go experts:

-   âœ… .NET
-   âœ… Terraform
-   âœ… Node.js
-   âœ… Go

Each comes with predefined best practices & usage guidelines.

> ğŸ“š **Note:** These are currently read-only.

### âœï¸ 4. Can I create my own Expert?

Yes, you can! ğŸ™Œ

Hereâ€™s how it works:

-   ğŸ“ Define your own custom guidelines
-   ğŸ”— Add up to 3 document links as references
-   ğŸ“¥ We scrape those URLs and convert the content to markdown
-   ğŸ§  LLM uses both the guidelines + docs for code generation

> âœ… **Result?** A highly tailored, smart assistant.

### âš™ï¸ 5. How does it work under the hood?

When you select an Expert, the system sends:

-   The guidelines
-   The scraped doc content

...to the LLM, giving it the context it needs to generate better code.

<br />

<div align="center">
	<img src="../../assets/gifs/experts.gif" alt="Expert Selection and Context Flow" />
	<p>
		<i>Experts</i>
	</p>
</div>

### ğŸš§ 6. Current Limitations

-   ğŸ“„ Only 3 documents can be uploaded right now
-   ğŸŒ Large URLs may not be fully processed

### ğŸ” 7. Are we solving this?

Yes! We're actively exploring:

-   Smarter ways to index and chunk content
-   Reducing noise while keeping the useful stuff

### ğŸ§ª 8. How we handle doc content?

We tested two approaches:

#### ğŸ“„ Raw Scraped Content

**Pros:**

-   Full fidelity

**Cons:**

-   Too much info = LLM overwhelm
-   Higher chance of hitting token limits

#### âœ‚ï¸ Summarized via LLM (Current Approach)

**Pros:**

-   Clean, focused content
-   Smaller token usage

**Cons:**

-   Might skip minor details

> âš–ï¸ **Trade-off:** Better performance with most important info kept intact.

### ğŸ“„ 9. Expert Guidelines

Looking to define your own expert? Check out the [Experts Guidelines](./experts-guidelines.mdx) file.
